# Roundâ€‘1B PDFÂ Scorer  
Semantic + Lexical + Crossâ€‘Encoder Rerank
========================================

This repository contains a **singleâ€‘file pipeline (`main.py`)** that selects the
most relevant PDF sections for a given *personaâ€¯+â€¯task* (ChallengeÂ 1â€¯B).

| Component | Model | Size | Purpose |
|-----------|-------|------|---------|
| Base encoder | `gtrâ€‘t5â€‘base` | â‰ˆâ€¯540â€¯MB | dense semantic recall |
| Reranker | `crossâ€‘encoder/msâ€‘marcoâ€‘MiniLMâ€‘Lâ€‘6â€‘v2` | â‰ˆâ€¯65â€¯MB | pairâ€‘wise relevance rerank |
| Lexical | BM25 (pure Python) | â€“ | keyâ€‘term recall + tieâ€‘breaker |

The code runs **entirely on CPU, without Internet access**, once the models are
embedded in the Docker image.

---

## 1Â Â ðŸ—Â Build the Docker image (first time only)

> **Headsâ€‘up:** the first `docker build` **is slow (~8â€¯min)** because it must:
>
> * download â‰ˆâ€¯300â€¯MB of Python wheels (`torch`, `transformers`, â€¦)  
> * download and unpack â‰ˆâ€¯600â€¯MB of HF model tarballs
>

```bash
# At repo root
docker build -t round1b .
````

*Inside the Dockerfile we fetch the 2 model tarballs from the projectâ€™s
GitHubÂ â†’Â Releases page; they do **not** live in git, so the repo stays small.*

---

### 2Â Â Run the scorer
##### Note: Please use /data with output else your output will be lost

> **Mounting your files**
> `-v <hostâ€‘path>:/data` tells Docker to **treat whatever folder you point at asÂ `/data` inside the container**.
> Just replace `<hostâ€‘path>` with the folder that already contains
> `challenge1b_input.json`Â and theâ€¯`PDFs/` subâ€‘directory.

#### 2.1Â LinuxÂ /Â macOSÂ example

```bash
# Inside the repo root â€“ adjust if your files live elsewhere
docker run --rm \
  -v "$PWD/Challenge_1b/Collection_2":/data \   # <â€‘â€‘ your host folder
  round1b \
  --challenge_json /data/challenge1b_input.json \
  --pdf_dir        /data/PDFs \
  --output         /data/challenge1b_output.json
```

#### 2.2Â WindowsÂ (PowerShellÂ orÂ CMD)Â example

```powershell
REM Replace the path before the colon with YOUR folder:
docker run --rm ^
  -v "C:\Users\you\Challenge_1b\Collection_2":/data ^
  round1b ^
  --challenge_json /data/challenge1b_input.json ^
  --pdf_dir        /data/PDFs ^
  --output         /data/challenge1b_output.json
```

If everything is mounted correctly the container will report:

```
[INFO] done in 38.7s â€“  kept 5 sections
âœ“ wrote /data/challenge1b_output.json
```

â€” and `challenge1b_output.json` will appear inside the same host folder you
mounted.

## 3Â Â CLI options (inside the container)

```
--challenge_json   required   path to the input JSON
--pdf_dir          required   directory containing the PDFs
--output           default=challenge1b_output.json
--model_path       default=/app/models/sentence-transformers/gtr-t5-base
--cross_model      default=/app/models/cross-encoder/ms-marco-MiniLM-L-6-v2
--top_out          default=15  hard cap on #sections in output
```

You normally **donâ€™t touchâ€¯`--model_path`â€¯/â€¯`--cross_model`** because the models
are baked into the image.

---

## 4Â Â ðŸš€Â How the pipeline works (plain English)

1. **Turn the prompt into a query**
   *â€œHR professional needs to create and manage fillable forms â€¦â€* becomes a
   single sentence.

2. **Slice every PDF into candidate sections**
   Using the outline headings we grab each heading + the next 1â€‘2Â pages of text.
   Generic headings like *â€œIntroductionâ€* are discarded.

3. **Recall phase**

   * 300â€¯candidates are fetched by **BM25** (keyword overlap).
   * Each gets a 768â€‘d embedding from **gtrâ€‘t5â€‘base** and we compute cosine
     similarity â†’ hybrid score = `0.9*dense + 0.1*BM25`.

4. **Adaptive elbow cut**
   We keep candidates until the score curve drops sharply or falls below
   0.45â€¯(similarity). Usually 15â€‘30 remain.

5. **Rerank phase**
   Each remaining `(query, sectionâ€‘preview)` pair is passed through the
   **crossâ€‘encoder**, which reads both texts and emits a fineâ€‘grained relevance
   score.
   Final score = `0.6*cross + 0.4*hybrid`.

6. **Output**
   The topâ€‘`N` (defaultâ€¯5) sections are dumped to JSON:

   * `extracted_sections`Â â†’ doc name, heading, rank, page
   * `subsection_analysis`Â â†’ 1â€‘paragraph â€œleadâ€‘summaryâ€ for quick preview.

Total runtime on 6â€¯Ã—â€¯200â€‘page PDFs: **35â€‘50â€¯s** on an 8â€‘vCPU laptop, CPU only.
